{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow_core.python.keras.layers.pooling import GlobalAveragePooling2D\n",
    "from tensorflow_core.python.layers.convolutional import Conv2D\n",
    "from tensorflow_core.python.layers.core import Dense\n",
    "from notebooks.utils import show_images, gaussian_filter, image_normalization, rescale, read_image\n",
    "import imquality.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'tensorflow version {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset that we are going to use to train and test this algorithm is [LiveIQA](https://live.ece.utexas.edu/research/quality/subjective.htm).\n",
    "It is comprised of 30 reference images, and 5 different distortions with 5 severity levels each.\n",
    "\n",
    "The first thing we need to do is to download the dataset. For this, I have created a couple of builders\n",
    "for Image Quality datasets in the [image-quality](https://github.com/ocampor/image-quality) package. The builders\n",
    "are an interface defined by tensorflow in [tensorflow-datasets](https://www.tensorflow.org/datasets) package. \n",
    "\n",
    "This process is going to take a couple of minutes because the dataset size is around 700 megabytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "builder = imquality.datasets.LiveIQA()\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After downloading and preparing the data, we can turn the builder as a dataset and shuffle it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds = builder.as_dataset()['train']\n",
    "ds = ds.shuffle(1024).batch(1).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The output is a generator; therefore, we cannot access it unless we iterate in a for loop. In order to display an\n",
    "image, I am iterating once to extract a sample. You can iterate this several times to understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for features in  ds.take(1):\n",
    "    distorted_image = features['distorted_image']\n",
    "    reference_image = features['reference_image']\n",
    "    dmos = tf.round(features['dmos'][0], 2)\n",
    "    distortion = features['distortion'][0]\n",
    "    print(f'The distortion of the image is {dmos} with'\n",
    "          f' a distortion {distortion}')\n",
    "    show_images([reference_image, distorted_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Image Normalization\n",
    "\n",
    "As pre-processing the image is turned into grayscale. As a second calculation, a low pass filter is applied\n",
    "to the grayscale image. Finally, the low-pass filtered image is subtracted from the grayscale image. The\n",
    "low frequency image is the result of blurring the image, downscaling by a factor of 1 / 4 and upscaling back\n",
    "to the original size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\hat{I} = I_{gray} - I^{low}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The main reasons for the image normalization are:\n",
    "1. The Human Visual System (HVS) is not sensitive to changes in low frequency band.\n",
    "\n",
    "2. Image distortions barely affect the low-frequency component of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def image_preprocess(image: tf.Tensor) -> tf.Tensor:\n",
    "    assert isinstance(image, tf.Tensor), 'The input must be a tf.Tensor'\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image_low = gaussian_filter(image, 16, 7 / 6)\n",
    "    image_low = rescale(image_low, 1 / 4, method=tf.image.ResizeMethod.BICUBIC)\n",
    "    image_low = rescale(image_low, 4, method=tf.image.ResizeMethod.BICUBIC)\n",
    "    return image - tf.cast(image_low, image.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for features in ds.take(1):\n",
    "    distorted_image = features['distorted_image']\n",
    "    reference_image = features['reference_image']\n",
    "    I_d = image_preprocess(distorted_image)\n",
    "    I_d = tf.image.grayscale_to_rgb(I_d)\n",
    "    I_d = image_normalization(I_d, 0, 1)\n",
    "\n",
    "show_images([reference_image, I_d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Error Map\n",
    "\n",
    "In the first stage of training, the objective error maps are used as proxy regression targets to get the effect of \n",
    "increasing data. The loss function is defined by the mean squared error between the predicted and ground-truth error\n",
    "maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{e}_{gt} = err(\\hat{I}_r, \\hat{I}_d)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $err(\\cdot)$ is any error function. The authors decided to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{e}_{gt} = | \\hat{I}_r -  \\hat{I}_d | ^ p\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $p=0.2$ in order to prevent that the values in the error map are small or close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def error_map(reference: tf.Tensor, distorted: tf.Tensor, p: float=0.2) -> tf.Tensor:\n",
    "    assert reference.shape == distorted.shape, 'Both images must be of the same size'\n",
    "    assert reference.dtype == tf.float32 and distorted.dtype == tf.float32, 'dtype must be tf.float32'\n",
    "    return tf.pow(tf.abs(reference - distorted), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "I = tf.convert_to_tensor(imageio.imread(get_image_url(2, 11, 0)))\n",
    "I_r = image_preprocess(I)\n",
    "results = []\n",
    "for severity in (1, 3, 5):\n",
    "    I = tf.convert_to_tensor(imageio.imread(get_image_url(2, 11, severity)))\n",
    "    I_d = image_preprocess(I)\n",
    "    e_gt = error_map(I_r, I_d, 0.2)\n",
    "    e_gt = tf.image.grayscale_to_rgb(e_gt)\n",
    "    e_gt = image_normalization(e_gt, 0, 1)\n",
    "    results.append(e_gt)\n",
    "\n",
    "show_images(results, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability Map Prediction\n",
    "\n",
    "According to the author, the model is likely to fail to predict the objective error map of\n",
    "homogeneous regions without having information of its pristine image. Thus, he proposes a \n",
    "reliability function. The assumption is that blurry regions have lower reliability than textured \n",
    "regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{r} = \\frac{2}{1 + exp(-\\alpha|\\hat{I}_d|)} - 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where Î± controls the saturation property of the reliability map. To assign sufficiently\n",
    "large values to pixels with small values, the positive part of a sigmoid is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reliability_map(distorted: tf.Tensor, alpha: float) -> tf.Tensor:\n",
    "    assert distorted.dtype == tf.float32, 'The Tensor must by of dtype tf.float32'\n",
    "    return 2 / (1 + tf.exp(- alpha * tf.abs(distorted))) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, to prevent the reliability map to directly affect the predicted score,\n",
    "it is divided by its average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{\\hat{r}} = \\frac{1}{\\frac{1}{H_rW_r}\\sum_{(i,j)}\\mathbf{r}(i,j)}\\mathbf{r}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def average_reliability_map(distorted: tf.Tensor, alpha: float) -> tf.Tensor:\n",
    "    r = reliability_map(distorted, alpha)\n",
    "    return r / tf.reduce_mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for severity in (1, 3, 5):\n",
    "    I = tf.convert_to_tensor(imageio.imread(get_image_url(2, 11, severity)))\n",
    "    I_d = image_preprocess(I)\n",
    "    r = average_reliability_map(I_d, 1)\n",
    "    r = tf.image.grayscale_to_rgb(r)\n",
    "    results.append(image_normalization(r, 0, 1))\n",
    "\n",
    "show_images(results, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loss function\n",
    "The loss function is the mean square error of the product between the reliability map and the\n",
    "error. The error is the difference between the predicted error map and the ground-truth error map.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_1(\\hat{I}_d; \\theta_f, \\theta_g) = ||g(f(\\hat{I}_d, \\theta_f), \\theta_g) - \\mathbf{e}_{gt}) \\odot \\mathbf{\\hat{r}}||^2_2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files\n",
    "We don't want to mix reference images in train and test because we want to test with completly unseen samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "idx = list(range(1, 25))\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx = idx[0:22]\n",
    "test_idx = idx[21:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_paths(idxs: list, base_uri: str) -> list:\n",
    "    return [\n",
    "    (idx, get_image_url(idx, distortion, severity, base_uri))\n",
    "    for idx in idxs\n",
    "    for distortion in range(1, 24)\n",
    "    for severity in range(1, 5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "base_uri = '/Users/ricardoocampo/Data/tid2013'\n",
    "train_uris = get_paths(train_idx, base_uri)\n",
    "test_uris = get_paths(test_idx, base_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preproces_image(uri):\n",
    "    image = read_image(uri)\n",
    "    return image_preprocess(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_images = [\n",
    "    load_and_preproces_image(filepath)\n",
    "    for _, filepath in train_uris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.stack(train_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_y(idx, train_uris, train, base_uri):\n",
    "    e_gt = error_map(load_and_preproces_image(get_image_url(train_uris[idx][0], None, 0, base_uri)), train[idx])\n",
    "    return tf.image.resize(e_gt, (int(384/4), int(512/4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r(distorted):\n",
    "    r = average_reliability_map(distorted, 0.2)\n",
    "    return tf.image.resize(r, (int(384/4), int(512/4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [\n",
    "    calculate_y(idx, train_uris, train, base_uri)\n",
    "    for idx in range(len(train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [\n",
    "    calculate_r(t)\n",
    "    for t in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = tf.stack(train_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r = tf.stack(r, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input = tf.keras.Input(shape=(None, None, 1), batch_size=50, name='original_image')\n",
    "f = Conv2D(48, (3, 3), name='Conv1', activation='relu', padding='same')(input)\n",
    "f = Conv2D(48, (3, 3), name='Conv2', activation='relu', padding='same', strides=(2, 2))(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv3', activation='relu', padding='same')(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv4', activation='relu', padding='same', strides=(2, 2))(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv5', activation='relu', padding='same')(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv6', activation='relu', padding='same')(f)\n",
    "f = Conv2D(128, (3, 3), name='Conv7', activation='relu', padding='same')(f)\n",
    "f = Conv2D(128, (3, 3), name='Conv8', activation='relu', padding='same')(f)\n",
    "g = Conv2D(1, (1, 1), name='Conv9', padding='same', activation='linear')(f)\n",
    "objective_error_map = tf.keras.Model(input, g, name='objective_error_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def diqa_loss_1(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred) * weights)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Nadam(learning_rate=2 * 10 ** -4)\n",
    "objective_error_map.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.losses.MeanSquaredError(),\n",
    "    metrics=[tf.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "objective_error_map.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = objective_error_map.fit(train, train_y,\n",
    "                    batch_size=50,\n",
    "                    epochs=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "idx = 230\n",
    "test_x = load_and_preproces_image(test_uris[idx][1])\n",
    "images.append(image_normalization(tf.squeeze(test_x), 0, 1))\n",
    "x = objective_error_map.predict(test_x[tf.newaxis, :, :, :])\n",
    "images.append(image_normalization(tf.squeeze(x), 0, 1))\n",
    "reference = load_and_preproces_image(get_image_url(test_uris[idx][0], None, 0, base_uri))\n",
    "e_gt = error_map(reference, test_x)\n",
    "images.append(image_normalization(tf.squeeze(e_gt), 0, 1))\n",
    "show_images(images, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mos = open('/Users/ricardoocampo/Data/tid2013/mos_with_names.txt', 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mos = [x.split(' ') for x in mos]\n",
    "mos = {y.lower().replace('\\n', ''):x for x, y in mos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(x):\n",
    "    return x.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mos_arr = [float(mos[get_file_name(train_uri)]) for _, train_uri in train_uris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mos_y = tf.convert_to_tensor(mos_arr, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = GlobalAveragePooling2D(data_format='channels_last')(f)\n",
    "h = Dense(128, activation='relu')(v)\n",
    "h = Dense(128, activation='relu')(h)\n",
    "h = Dense(1)(h)\n",
    "subjective_error = tf.keras.Model(input, h, name='subjective_error')\n",
    "\n",
    "optimizer = tf.optimizers.Nadam(learning_rate=2 * 10 ** -4)\n",
    "subjective_error.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.losses.MeanSquaredError(),\n",
    "    metrics=[tf.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_error.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = subjective_error.fit(train, mos_y,\n",
    "                    batch_size=50,\n",
    "                    epochs=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "idx = 170\n",
    "test_x = load_and_preproces_image(test_uris[idx][1])\n",
    "images.append(image_normalization(tf.squeeze(test_x), 0, 1))\n",
    "prediction = subjective_error.predict(test_x[tf.newaxis, :, :, :])[0][0]\n",
    "target = float(mos[get_file_name(test_uris[idx][1])])\n",
    "\n",
    "print(f'the predicted value is: {prediction} and target is: {target}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
