{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, GlobalAveragePooling2D\n",
    "from notebooks.utils import show_images, gaussian_filter, image_normalization, rescale, image_shape\n",
    "import imquality.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'tensorflow version {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset that we are going to use to train and test this algorithm is [LiveIQA](https://live.ece.utexas.edu/research/quality/subjective.htm).\n",
    "It is comprised of 30 reference images, and 5 different distortions with 5 severity levels each.\n",
    "\n",
    "The first thing we need to do is to download the dataset. For this, I have created a couple of builders\n",
    "for Image Quality datasets in the [image-quality](https://github.com/ocampor/image-quality) package. The builders\n",
    "are an interface defined by tensorflow in [tensorflow-datasets](https://www.tensorflow.org/datasets) package. \n",
    "\n",
    "This process is going to take a couple of minutes because the dataset size is around 700 megabytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "builder = imquality.datasets.LiveIQA()\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After downloading and preparing the data, we can turn the builder as a dataset and shuffle it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds = builder.as_dataset(shuffle_files=True)['train']\n",
    "ds = ds.shuffle(1024).batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The output is a generator; therefore, we cannot access it unless we iterate in a for loop. In order to display an\n",
    "image, I am iterating once to extract a sample. You can iterate this several times to understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for features in  ds.take(2):\n",
    "    distorted_image = features['distorted_image']\n",
    "    reference_image = features['reference_image']\n",
    "    dmos = tf.round(features['dmos'][0], 2)\n",
    "    distortion = features['distortion'][0]\n",
    "    print(f'The distortion of the image is {dmos} with'\n",
    "          f' a distortion {distortion} and shape {distorted_image.shape}')\n",
    "    show_images([reference_image, distorted_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Image Normalization\n",
    "\n",
    "As pre-processing the image is turned into grayscale. As a second calculation, a low pass filter is applied\n",
    "to the grayscale image. Finally, the low-pass filtered image is subtracted from the grayscale image. The\n",
    "low frequency image is the result of blurring the image, downscaling by a factor of 1 / 4 and upscaling back\n",
    "to the original size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\hat{I} = I_{gray} - I^{low}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The main reasons for the image normalization are:\n",
    "1. The Human Visual System (HVS) is not sensitive to changes in low frequency band.\n",
    "\n",
    "2. Image distortions barely affect the low-frequency component of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def image_preprocess(image: tf.Tensor) -> tf.Tensor:\n",
    "    assert isinstance(image, tf.Tensor), 'The input must be a tf.Tensor'\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image_low = gaussian_filter(image, 16, 7 / 6)\n",
    "    image_low = rescale(image_low, 1 / 4, method=tf.image.ResizeMethod.BICUBIC)\n",
    "    image_low = tf.image.resize(image_low, size=image_shape(image), method=tf.image.ResizeMethod.BICUBIC)\n",
    "    return image - tf.cast(image_low, image.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for features in ds.take(2):\n",
    "    distorted_image = features['distorted_image']\n",
    "    reference_image = features['reference_image']\n",
    "    I_d = image_preprocess(distorted_image)\n",
    "    I_d = tf.image.grayscale_to_rgb(I_d)\n",
    "    I_d = image_normalization(I_d, 0, 1)\n",
    "    show_images([reference_image, I_d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Error Map\n",
    "\n",
    "In the first stage of training, the objective error maps are used as proxy regression targets to get the effect of \n",
    "increasing data. The loss function is defined by the mean squared error between the predicted and ground-truth error\n",
    "maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{e}_{gt} = err(\\hat{I}_r, \\hat{I}_d)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $err(\\cdot)$ is any error function. The authors decided to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{e}_{gt} = | \\hat{I}_r -  \\hat{I}_d | ^ p\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $p=0.2$ in order to prevent that the values in the error map are small or close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def error_map(reference: tf.Tensor, distorted: tf.Tensor, p: float=0.2) -> tf.Tensor:\n",
    "    assert reference.dtype == tf.float32 and distorted.dtype == tf.float32, 'dtype must be tf.float32'\n",
    "    return tf.pow(tf.abs(reference - distorted), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for features in ds.take(3):\n",
    "    reference_image = features['reference_image'] \n",
    "    I_r = image_preprocess(reference_image)\n",
    "    I_d = image_preprocess(features['distorted_image'])\n",
    "    e_gt = error_map(I_r, I_d, 0.2)\n",
    "    I_d = image_normalization(tf.image.grayscale_to_rgb(I_d), 0, 1)\n",
    "    e_gt = image_normalization(tf.image.grayscale_to_rgb(e_gt), 0, 1)\n",
    "    show_images([reference_image, I_d, e_gt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability Map Prediction\n",
    "\n",
    "According to the author, the model is likely to fail to predict the objective error map of\n",
    "homogeneous regions without having information of its pristine image. Thus, he proposes a \n",
    "reliability function. The assumption is that blurry regions have lower reliability than textured \n",
    "regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{r} = \\frac{2}{1 + exp(-\\alpha|\\hat{I}_d|)} - 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where Î± controls the saturation property of the reliability map. To assign sufficiently\n",
    "large values to pixels with small values, the positive part of a sigmoid is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reliability_map(distorted: tf.Tensor, alpha: float) -> tf.Tensor:\n",
    "    assert distorted.dtype == tf.float32, 'The Tensor must by of dtype tf.float32'\n",
    "    return 2 / (1 + tf.exp(- alpha * tf.abs(distorted))) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, to prevent the reliability map to directly affect the predicted score,\n",
    "it is divided by its average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{\\hat{r}} = \\frac{1}{\\frac{1}{H_rW_r}\\sum_{(i,j)}\\mathbf{r}(i,j)}\\mathbf{r}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def average_reliability_map(distorted: tf.Tensor, alpha: float) -> tf.Tensor:\n",
    "    r = reliability_map(distorted, alpha)\n",
    "    return r / tf.reduce_mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features in ds.take(2):\n",
    "    reference_image = features['reference_image'] \n",
    "    I_d = image_preprocess(features['distorted_image'])\n",
    "    r = average_reliability_map(I_d, 1)\n",
    "    r = image_normalization(tf.image.grayscale_to_rgb(r), 0, 1)\n",
    "    show_images([reference_image, r], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loss function\n",
    "The loss function is the mean square error of the product between the reliability map and the\n",
    "error. The error is the difference between the predicted error map and the ground-truth error map.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_1(\\hat{I}_d; \\theta_f, \\theta_g) = ||g(f(\\hat{I}_d, \\theta_f), \\theta_g) - \\mathbf{e}_{gt}) \\odot \\mathbf{\\hat{r}}||^2_2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files\n",
    "We don't want to mix reference images in train and test because we want to test with completly unseen samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_map(features):\n",
    "    I_d = image_preprocess(features['distorted_image'])\n",
    "    I_r = image_preprocess(features['reference_image'])\n",
    "    r = rescale(average_reliability_map(I_d, 0.2), 1 / 4)\n",
    "    e_gt = rescale(error_map(I_r, I_d, 0.2), 1 / 4)\n",
    "    return (I_d, e_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds.map(calculate_error_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input = tf.keras.Input(shape=(None, None, 1), batch_size=1, name='original_image')\n",
    "f = Conv2D(48, (3, 3), name='Conv1', activation='relu', padding='same')(input)\n",
    "f = Conv2D(48, (3, 3), name='Conv2', activation='relu', padding='same', strides=(2, 2))(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv3', activation='relu', padding='same')(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv4', activation='relu', padding='same', strides=(2, 2))(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv5', activation='relu', padding='same')(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv6', activation='relu', padding='same')(f)\n",
    "f = Conv2D(128, (3, 3), name='Conv7', activation='relu', padding='same')(f)\n",
    "f = Conv2D(128, (3, 3), name='Conv8', activation='relu', padding='same')(f)\n",
    "g = Conv2D(1, (1, 1), name='Conv9', padding='same', activation='linear')(f)\n",
    "objective_error_map = tf.keras.Model(input, g, name='objective_error_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def diqa_loss_1(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred) * weights)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Nadam(learning_rate=2 * 10 ** -4)\n",
    "objective_error_map.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.losses.MeanSquaredError(),\n",
    "    metrics=[tf.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "objective_error_map.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = objective_error_map.fit(x=train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = GlobalAveragePooling2D(data_format='channels_last')(f)\n",
    "h = Dense(128, activation='relu')(v)\n",
    "h = Dense(128, activation='relu')(h)\n",
    "h = Dense(1)(h)\n",
    "subjective_error = tf.keras.Model(input, h, name='subjective_error')\n",
    "\n",
    "optimizer = tf.optimizers.Nadam(learning_rate=2 * 10 ** -4)\n",
    "subjective_error.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.losses.MeanSquaredError(),\n",
    "    metrics=[tf.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_error.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subjective_score(features):\n",
    "    I_d = image_preprocess(features['distorted_image'])\n",
    "    mos = features['dmos']\n",
    "    return (I_d, mos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds.map(calculate_subjective_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = subjective_error.fit(train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(ds))\n",
    "I_d = image_preprocess(sample['distorted_image'])\n",
    "target = sample['dmos'][0]\n",
    "prediction = subjective_error.predict(I_d)[0][0]\n",
    "\n",
    "print(f'the predicted value is: {prediction} and target is: {target}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
