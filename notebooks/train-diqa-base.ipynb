{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Conv2D\n",
    "from utils import show_images, gaussian_filter, image_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset that we are going to use to train and test this algorithm is [TID2013](http://www.ponomarenko.info/tid2013.htm).\n",
    "It is comprised of 25 reference images, and 24 different distortions with 5 severy levels each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_url(image_idx: int, distortion: int, severity: int) -> str:\n",
    "    if severity == 0:\n",
    "        image_type = 'reference_images'\n",
    "        image_path = f'i{image_idx:02}.bmp'\n",
    "    else:\n",
    "        image_type = 'distorted_images'\n",
    "        image_path = f'i{image_idx:02}_{distortion:02}_{severity}.bmp'\n",
    "        \n",
    "    return f'https://data.ocampor.ai/image-quality/tid2013/{image_type}/{image_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for idx in (2, 8, 21):\n",
    "    images = []\n",
    "    for distortion in (3, 8, 10, 11):\n",
    "        images.append(imageio.imread(get_image_url(idx, distortion, 5)))\n",
    "    show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Image Normalization\n",
    "\n",
    "As pre-processing the image is turned into grayscale. As a second calculation, a low pass filter is applied\n",
    "to the grayscale image. Finally, the low-pass filtered image is subtracted from the grayscale image. The\n",
    "low frequency image is the result of blurring the image, downscaling by a factor of 1 / 4 and upscaling back\n",
    "to the original size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\hat{I} = I_{gray} - I^{low}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The main reasons for the image normalization are:\n",
    "1. The Human Visual System (HVS) is not sensitive to changes in low frequency band.\n",
    "2. Image distortions barely affect the low-frequency component of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def image_preprocess(image: tf.Tensor) -> tf.Tensor:\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    shape = image.get_shape()\n",
    "    rescale_size = tf.convert_to_tensor(shape[:2], dtype=tf.int32) / 4\n",
    "    \n",
    "    image_low = gaussian_filter(image, 16, 7 / 6)\n",
    "    image_low = tf.image.resize(\n",
    "        image_low,\n",
    "        tf.cast(rescale_size, tf.int32),\n",
    "        tf.image.ResizeMethod.BICUBIC)\n",
    "    image_low = tf.image.resize(\n",
    "        image_low,\n",
    "        tf.cast(shape[:2], tf.int32),\n",
    "        tf.image.ResizeMethod.BICUBIC)\n",
    "    return image - tf.reshape(image_low, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for severity in (1, 3, 5):\n",
    "    I_d = image_preprocess(imageio.imread(get_image_url(2, 11, severity)))\n",
    "    I_d = image_normalization(tf.image.grayscale_to_rgb(I_d), 0, 1)\n",
    "    results.append(I_d)\n",
    "\n",
    "show_images(results, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Error Map\n",
    "\n",
    "In the first stage of training, the objective error maps are used as proxy regression targets to get the effect of \n",
    "increasing data. The loss function is defined by the mean squared error between the predicted and ground-truth error\n",
    "maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{e}_{gt} = err(\\hat{I}_r, \\hat{I}_d)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $err(\\cdot)$ is any error function. The authors decided to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{e}_{gt} = | \\hat{I}_r -  \\hat{I}_d | ^ p\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $p=0.2$ in order to prevent that the values in the error map are small or close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def error_map(reference: tf.Tensor, distorted: tf.Tensor, p: float=0.2) -> tf.Tensor:\n",
    "    assert reference.shape == distorted.shape, 'Both images must be of the same size'\n",
    "    return tf.pow(tf.abs(reference - distorted), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "I_r = image_preprocess(imageio.imread(get_image_url(2, 11, 0)))\n",
    "results = []\n",
    "for severity in (1, 3, 5):\n",
    "    I_d = image_preprocess(imageio.imread(get_image_url(2, 11, severity)))\n",
    "    e_gt = error_map(I_r, I_d, 0.2)\n",
    "    e_gt = image_normalization(tf.image.grayscale_to_rgb(e_gt), 0, 1)\n",
    "    results.append(e_gt)\n",
    "\n",
    "show_images(results, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability Map Prediction\n",
    "\n",
    "According to the author, the model is likely to fail to predict the objective error map of\n",
    "homogeneous regions without having information of its pristine image. Thus, he proposes a \n",
    "reliability function. The assumption is that blurry regions have lower reliability than textured \n",
    "regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{r} = \\frac{2}{1 + exp(-\\alpha|\\hat{I}_d|)} - 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where Î± controls the saturation property of the reliability map. To assign sufficiently\n",
    "large values to pixels with small values, the positive part of a sigmoid is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def reliability_map(distorted: tf.Tensor, alpha: float) -> tf.Tensor:\n",
    "    assert distorted.dtype == tf.float32, 'The Tensor must by of dtype tf.float32'\n",
    "    return 2 / (1 + tf.exp(- alpha * tf.abs(distorted))) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, to prevent the reliability map to directly affect the predicted score,\n",
    "it is divided by its average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbf{\\hat{r}} = \\frac{1}{\\frac{1}{H_rW_r}\\sum_{(i,j)}\\mathbf{r}(i,j)}\\mathbf{r}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def average_reliability_map(distorted: tf.Tensor, alpha: float) -> tf.Tensor:\n",
    "    r = reliability_map(distorted, alpha)\n",
    "    return r / tf.reduce_mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for severity in (1, 3, 5):\n",
    "    I_d = image_preprocess(imageio.imread(get_image_url(2, 11, severity)))\n",
    "    r = average_reliability_map(I_d, 1)\n",
    "    r = image_normalization(tf.image.grayscale_to_rgb(r), 0, 1)\n",
    "    results.append(r)\n",
    "\n",
    "show_images(results, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loss function\n",
    "The loss function is the mean square error of the product between the reliability map and the\n",
    "error. The error is the difference between the predicted error map and the ground-truth error map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathcal{L}_1(\\hat{I}_d; \\theta_f, \\theta_g) = ||g(f(\\hat{I}_d, \\theta_f), \\theta_g) - \\mathbf{e}_{gt}) \\odot \\mathbf{\\hat{r}}||^2_2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32').reshape(60000, 28, 28, 1) / 255\n",
    "x_test = x_test.astype('float32').reshape(10000, 28, 28, 1) / 255\n",
    "y_train = tf.image.resize(x_train, [7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input = tf.keras.Input(shape=(None, None, 1), batch_size=1, name='original_image')\n",
    "f = Conv2D(48, (3, 3), name='Conv1', activation='relu', padding='same')(input)\n",
    "f = Conv2D(48, (3, 3), name='Conv2', activation='relu', padding='same', strides=(2, 2))(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv3', activation='relu', padding='same')(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv4', activation='relu', padding='same', strides=(2, 2))(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv5', activation='relu', padding='same')(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv6', activation='relu', padding='same')(f)\n",
    "f = Conv2D(128, (3, 3), name='Conv7', activation='relu', padding='same')(f)\n",
    "f = Conv2D(128, (3, 3), name='Conv8', activation='relu', padding='same')(f)\n",
    "g = Conv2D(1, (1, 1), name='Conv9', padding='same', activation='linear')(f)\n",
    "objective_error_map = tf.keras.Model(input, g, name='objective_error_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Nadam(learning_rate=2 * 10 ** -4)\n",
    "objective_error_map.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.losses.MeanSquaredError(),\n",
    "    metrics=[tf.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "objective_error_map.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = objective_error_map.fit(x_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_test[10:11].reshape([28, 28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = objective_error_map.predict(x_test[10:11])\n",
    "plt.imshow(x.reshape([7, 7]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
